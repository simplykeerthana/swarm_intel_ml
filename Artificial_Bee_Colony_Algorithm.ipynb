{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c70b67e3",
   "metadata": {},
   "source": [
    "# Feature selection using Artificial Bee Colony Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa855580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Artificial Bee Colony optimization to find an optimal subset of features for a ML classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964239c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from niapy.problems import Problem\n",
    "from niapy.task import Task\n",
    "from niapy.algorithms.basic import ArtificialBeeColonyAlgorithm\n",
    "import pandas as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b21db53b-1edb-4e50-802b-6355b2413a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNBFeatureSelection(Problem):\n",
    "    def __init__(self, X_train, y_train, alpha=0.99):\n",
    "        super().__init__(dimension=X_train.shape[1], lower=0, upper=1)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        selected = x > 0.5\n",
    "        num_selected = selected.sum()\n",
    "        if num_selected == 0:\n",
    "            return 1.0\n",
    "        accuracy = cross_val_score(GaussianNB(), self.X_train[:, selected], self.y_train, cv=2, n_jobs=-1).mean()\n",
    "        score = 1 - accuracy\n",
    "        num_features = self.X_train.shape[1]\n",
    "        return self.alpha * score + (1 - self.alpha) * (num_selected / num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d305d004-c696-4f98-9d49-47084ef34642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionFeatureSelection(Problem):\n",
    "    def __init__(self, X_train, y_train, alpha=0.99):\n",
    "        super().__init__(dimension=X_train.shape[1], lower=0, upper=1)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        selected = x > 0.5\n",
    "        num_selected = selected.sum()\n",
    "        if num_selected == 0:\n",
    "            return 1.0\n",
    "        accuracy = cross_val_score(LogisticRegression(), self.X_train[:, selected], self.y_train, cv=2, n_jobs=-1).mean()\n",
    "        score = 1 - accuracy\n",
    "        num_features = self.X_train.shape[1]\n",
    "        return self.alpha * score + (1 - self.alpha) * (num_selected / num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19313e7d-f0ed-4a62-87cc-ede0b1d0b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifierFeatureSelection(Problem):\n",
    "    def __init__(self, X_train, y_train, alpha=0.99):\n",
    "        super().__init__(dimension=X_train.shape[1], lower=0, upper=1)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        selected = x > 0.5\n",
    "        num_selected = selected.sum()\n",
    "        if num_selected == 0:\n",
    "            return 1.0\n",
    "        accuracy = cross_val_score(DecisionTreeClassifier(), self.X_train[:, selected], self.y_train, cv=2, n_jobs=-1).mean()\n",
    "        score = 1 - accuracy\n",
    "        num_features = self.X_train.shape[1]\n",
    "        return self.alpha * score + (1 - self.alpha) * (num_selected / num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f777b4bc-b650-4ed5-b09a-bd287fc87452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNeighborsClassifierFeatureSelection(Problem):\n",
    "    def __init__(self, X_train, y_train, alpha=0.99):\n",
    "        super().__init__(dimension=X_train.shape[1], lower=0, upper=1)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        selected = x > 0.5\n",
    "        num_selected = selected.sum()\n",
    "        if num_selected == 0:\n",
    "            return 1.0\n",
    "        accuracy = cross_val_score(KNeighborsClassifier(), self.X_train[:, selected], self.y_train, cv=2, n_jobs=-1).mean()\n",
    "        score = 1 - accuracy\n",
    "        num_features = self.X_train.shape[1]\n",
    "        return self.alpha * score + (1 - self.alpha) * (num_selected / num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c4c3d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMFeatureSelection(Problem):\n",
    "    def __init__(self, X_train, y_train, alpha=0.99):\n",
    "        super().__init__(dimension=X_train.shape[1], lower=0, upper=1)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _evaluate(self, x):\n",
    "        selected = x > 0.5\n",
    "        num_selected = selected.sum()\n",
    "        if num_selected == 0:\n",
    "            return 1.0\n",
    "        accuracy = cross_val_score(SVC(), self.X_train[:, selected], self.y_train, cv=2, n_jobs=-1).mean()\n",
    "        score = 1 - accuracy\n",
    "        num_features = self.X_train.shape[1]\n",
    "        return self.alpha * score + (1 - self.alpha) * (num_selected / num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd9beb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Number of selected features: 9\n",
      "Selected features: mean texture, mean smoothness, concavity error, fractal dimension error, worst radius, worst texture, worst perimeter, worst concave points, worst symmetry\n",
      "GaussianNBSubset accuracy: 0.9649122807017544\n",
      "GaussianNBAll Features Accuracy: 0.9649122807017544\n",
      "--------------------------------------------------\n",
      "Number of selected features: 13\n",
      "Selected features: mean radius, mean texture, mean smoothness, perimeter error, compactness error, concavity error, concave points error, symmetry error, worst texture, worst area, worst smoothness, worst concavity, worst symmetry\n",
      "DecisionTreeClassifierSubset accuracy: 0.9649122807017544\n",
      "DecisionTreeClassifierAll Features Accuracy: 0.956140350877193\n",
      "--------------------------------------------------\n",
      "Number of selected features: 5\n",
      "Selected features: mean texture, mean perimeter, mean concavity, worst texture, worst perimeter\n",
      "KNeighborsClassifierSubset accuracy: 0.9385964912280702\n",
      "KNeighborsClassifierAll Features Accuracy: 0.9210526315789473\n",
      "--------------------------------------------------\n",
      "Number of selected features: 5\n",
      "Selected features: mean radius, compactness error, worst texture, worst perimeter, worst concave points\n",
      "SVCSubset accuracy: 0.956140350877193\n",
      "SVCAll Features Accuracy: 0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "dataset = load_breast_cancer()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "feature_names = dataset.feature_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1234)\n",
    "\n",
    "problem_nb = GaussianNBFeatureSelection(X_train, y_train)\n",
    "problem_lr = LogisticRegressionFeatureSelection(X_train, y_train)\n",
    "problem_dt = DecisionTreeClassifierFeatureSelection(X_train, y_train)\n",
    "problem_knn = KNeighborsClassifierFeatureSelection(X_train, y_train)\n",
    "problem_svc = SVMFeatureSelection(X_train, y_train)\n",
    "\n",
    "#500 is the max number of iterations\n",
    "print(\"-\"*50)\n",
    "# GaussianNB\n",
    "task = Task(problem_nb, max_iters=200)\n",
    "\n",
    "algorithm = ArtificialBeeColonyAlgorithm(population_size=10, limit=100)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(feature_names[selected_features].tolist()))\n",
    "features1 = feature_names[selected_features].tolist()\n",
    "\n",
    "model_selected = GaussianNB()\n",
    "model_all = GaussianNB()\n",
    "model_name = \"GaussianNB\"\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print(model_name + 'Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "model_all.fit(X_train, y_train)\n",
    "print(model_name + 'All Features Accuracy:', model_all.score(X_test, y_test))\n",
    "print(\"-\"*50)\n",
    "# DecisionTreeClassifier\n",
    "task = Task(problem_dt, max_iters=200)\n",
    "\n",
    "algorithm = ArtificialBeeColonyAlgorithm(population_size=10, limit=100)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(feature_names[selected_features].tolist()))\n",
    "features1 = feature_names[selected_features].tolist()\n",
    "\n",
    "model_selected = DecisionTreeClassifier()\n",
    "model_all = DecisionTreeClassifier()\n",
    "model_name = \"DecisionTreeClassifier\"\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print(model_name + 'Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "model_all.fit(X_train, y_train)\n",
    "print(model_name + 'All Features Accuracy:', model_all.score(X_test, y_test))\n",
    "print(\"-\"*50)\n",
    "# KNeighborsClassifier\n",
    "task = Task(problem_knn, max_iters=200)\n",
    "\n",
    "algorithm = ArtificialBeeColonyAlgorithm(population_size=10, limit=100)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(feature_names[selected_features].tolist()))\n",
    "features1 = feature_names[selected_features].tolist()\n",
    "\n",
    "model_selected = KNeighborsClassifier()\n",
    "model_all = KNeighborsClassifier()\n",
    "model_name = \"KNeighborsClassifier\"\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print(model_name + 'Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "model_all.fit(X_train, y_train)\n",
    "print(model_name + 'All Features Accuracy:', model_all.score(X_test, y_test))\n",
    "print(\"-\"*50)\n",
    "# SVC\n",
    "task = Task(problem_svc, max_iters=200)\n",
    "\n",
    "algorithm = ArtificialBeeColonyAlgorithm(population_size=10, limit=100)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(feature_names[selected_features].tolist()))\n",
    "features1 = feature_names[selected_features].tolist()\n",
    "\n",
    "model_selected = SVC()\n",
    "model_all = SVC()\n",
    "model_name = \"SVC\"\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print(model_name + 'Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "model_all.fit(X_train, y_train)\n",
    "print(model_name + 'All Features Accuracy:', model_all.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e02fe8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Number of selected features: 9\n",
      "Selected features: mean texture, mean smoothness, mean compactness, fractal dimension error, worst radius, worst texture, worst area, worst smoothness, worst concave points\n",
      "GaussianNBSubset accuracy: 0.956140350877193\n",
      "GaussianNBAll Features Accuracy: 0.9649122807017544\n",
      "--------------------------------------------------\n",
      "Number of selected features: 17\n",
      "Selected features: mean radius, mean texture, mean smoothness, mean concave points, mean fractal dimension, radius error, perimeter error, area error, compactness error, concave points error, fractal dimension error, worst texture, worst area, worst smoothness, worst concavity, worst concave points, worst fractal dimension\n",
      "DecisionTreeClassifierSubset accuracy: 0.9473684210526315\n",
      "DecisionTreeClassifierAll Features Accuracy: 0.9473684210526315\n",
      "--------------------------------------------------\n",
      "Number of selected features: 4\n",
      "Selected features: mean radius, mean texture, smoothness error, worst perimeter\n",
      "KNeighborsClassifierSubset accuracy: 0.9649122807017544\n",
      "KNeighborsClassifierAll Features Accuracy: 0.9210526315789473\n",
      "--------------------------------------------------\n",
      "Number of selected features: 10\n",
      "Selected features: mean concavity, mean fractal dimension, radius error, texture error, smoothness error, fractal dimension error, worst texture, worst perimeter, worst compactness, worst concavity\n",
      "SVCSubset accuracy: 0.9473684210526315\n",
      "SVCAll Features Accuracy: 0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "# GaussianNB\n",
    "print(\"-\"*50)\n",
    "task = Task(problem_nb, max_iters=200)\n",
    "\n",
    "algorithm = ArtificialBeeColonyAlgorithm(population_size=20, limit=200)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(feature_names[selected_features].tolist()))\n",
    "features2 = feature_names[selected_features].tolist()\n",
    "\n",
    "model_selected = GaussianNB()\n",
    "model_all = GaussianNB()\n",
    "model_name = \"GaussianNB\"\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print(model_name + 'Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "\n",
    "model_all.fit(X_train, y_train)\n",
    "print(model_name + 'All Features Accuracy:', model_all.score(X_test, y_test))\n",
    "\n",
    "# DecisionTreeClassifier\n",
    "print(\"-\"*50)\n",
    "task = Task(problem_dt, max_iters=200)\n",
    "\n",
    "algorithm = ArtificialBeeColonyAlgorithm(population_size=20, limit=200)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(feature_names[selected_features].tolist()))\n",
    "features2 = feature_names[selected_features].tolist()\n",
    "\n",
    "model_selected = DecisionTreeClassifier()\n",
    "model_all = DecisionTreeClassifier()\n",
    "model_name = \"DecisionTreeClassifier\"\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print(model_name + 'Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "\n",
    "model_all.fit(X_train, y_train)\n",
    "print(model_name + 'All Features Accuracy:', model_all.score(X_test, y_test))\n",
    "\n",
    "# KNeighborsClassifier\n",
    "print(\"-\"*50)\n",
    "task = Task(problem_knn, max_iters=200)\n",
    "\n",
    "algorithm = ArtificialBeeColonyAlgorithm(population_size=20, limit=200)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(feature_names[selected_features].tolist()))\n",
    "features2 = feature_names[selected_features].tolist()\n",
    "\n",
    "model_selected = KNeighborsClassifier()\n",
    "model_all = KNeighborsClassifier()\n",
    "model_name = \"KNeighborsClassifier\"\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print(model_name + 'Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "\n",
    "model_all.fit(X_train, y_train)\n",
    "print(model_name + 'All Features Accuracy:', model_all.score(X_test, y_test))\n",
    "\n",
    "# SVC\n",
    "print(\"-\"*50)\n",
    "task = Task(problem_svc, max_iters=200)\n",
    "\n",
    "algorithm = ArtificialBeeColonyAlgorithm(population_size=20, limit=200)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(feature_names[selected_features].tolist()))\n",
    "features2 = feature_names[selected_features].tolist()\n",
    "\n",
    "model_selected = SVC()\n",
    "model_all = SVC()\n",
    "model_name = \"SVC\"\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print(model_name + 'Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "\n",
    "model_all.fit(X_train, y_train)\n",
    "print(model_name + 'All Features Accuracy:', model_all.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e87a321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Number of selected features: 3\n",
      "Selected features: worst texture, worst area, worst concave points\n",
      "GaussianNBSubset accuracy: 0.9473684210526315\n",
      "GaussianNBAll Features Accuracy: 0.9649122807017544\n",
      "--------------------------------------------------\n",
      "Number of selected features: 14\n",
      "Selected features: mean texture, mean perimeter, mean area, mean concavity, mean concave points, mean symmetry, texture error, area error, concave points error, worst radius, worst area, worst smoothness, worst concavity, worst concave points\n",
      "DecisionTreeClassifierSubset accuracy: 1.0\n",
      "DecisionTreeClassifierAll Features Accuracy: 0.956140350877193\n",
      "--------------------------------------------------\n",
      "Number of selected features: 6\n",
      "Selected features: mean texture, mean concave points, smoothness error, concave points error, worst radius, worst perimeter\n",
      "KNeighborsClassifierSubset accuracy: 0.9649122807017544\n",
      "KNeighborsClassifierAll Features Accuracy: 0.9210526315789473\n",
      "--------------------------------------------------\n",
      "Number of selected features: 4\n",
      "Selected features: mean radius, texture error, worst texture, worst perimeter\n",
      "SVCSubset accuracy: 0.956140350877193\n",
      "SVCAll Features Accuracy: 0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "# GaussianNB\n",
    "print(\"-\"*50)\n",
    "task = Task(problem_nb, max_iters=200)\n",
    "\n",
    "algorithm = ArtificialBeeColonyAlgorithm(population_size=50, limit=300)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(feature_names[selected_features].tolist()))\n",
    "features3 = feature_names[selected_features].tolist()\n",
    "\n",
    "model_selected = GaussianNB()\n",
    "model_all = GaussianNB()\n",
    "model_name = \"GaussianNB\"\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print(model_name + 'Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "model_all.fit(X_train, y_train)\n",
    "print(model_name + 'All Features Accuracy:', model_all.score(X_test, y_test))\n",
    "\n",
    "# DecisionTreeClassifier\n",
    "print(\"-\"*50)\n",
    "task = Task(problem_dt, max_iters=200)\n",
    "\n",
    "algorithm = ArtificialBeeColonyAlgorithm(population_size=50, limit=300)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(feature_names[selected_features].tolist()))\n",
    "features3 = feature_names[selected_features].tolist()\n",
    "\n",
    "model_selected = DecisionTreeClassifier()\n",
    "model_all = DecisionTreeClassifier()\n",
    "model_name = \"DecisionTreeClassifier\"\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print(model_name + 'Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "model_all.fit(X_train, y_train)\n",
    "print(model_name + 'All Features Accuracy:', model_all.score(X_test, y_test))\n",
    "\n",
    "# KNeighborsClassifier\n",
    "print(\"-\"*50)\n",
    "task = Task(problem_knn, max_iters=200)\n",
    "\n",
    "algorithm = ArtificialBeeColonyAlgorithm(population_size=50, limit=300)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(feature_names[selected_features].tolist()))\n",
    "features3 = feature_names[selected_features].tolist()\n",
    "\n",
    "model_selected = KNeighborsClassifier()\n",
    "model_all = KNeighborsClassifier()\n",
    "model_name = \"KNeighborsClassifier\"\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print(model_name + 'Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "model_all.fit(X_train, y_train)\n",
    "print(model_name + 'All Features Accuracy:', model_all.score(X_test, y_test))\n",
    "\n",
    "# SVC\n",
    "print(\"-\"*50)\n",
    "task = Task(problem_svc, max_iters=200)\n",
    "\n",
    "algorithm = ArtificialBeeColonyAlgorithm(population_size=50, limit=300)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(feature_names[selected_features].tolist()))\n",
    "features3 = feature_names[selected_features].tolist()\n",
    "\n",
    "model_selected = SVC()\n",
    "model_all = SVC()\n",
    "model_name = \"SVC\"\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print(model_name + 'Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "model_all.fit(X_train, y_train)\n",
    "print(model_name + 'All Features Accuracy:', model_all.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85398fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Number of selected features: 7\n",
      "Selected features: mean texture, mean concave points, worst texture, worst area, worst smoothness, worst concave points, worst fractal dimension\n",
      "GaussianNBSubset accuracy: 0.956140350877193\n",
      "GaussianNBAll Features Accuracy: 0.9649122807017544\n",
      "--------------------------------------------------\n",
      "Number of selected features: 10\n",
      "Selected features: mean area, mean smoothness, mean fractal dimension, perimeter error, area error, smoothness error, concave points error, worst texture, worst area, worst compactness\n",
      "DecisionTreeClassifierSubset accuracy: 0.9210526315789473\n",
      "DecisionTreeClassifierAll Features Accuracy: 0.9649122807017544\n",
      "--------------------------------------------------\n",
      "Number of selected features: 5\n",
      "Selected features: mean radius, mean texture, perimeter error, worst radius, worst perimeter\n",
      "KNeighborsClassifierSubset accuracy: 0.956140350877193\n",
      "KNeighborsClassifierAll Features Accuracy: 0.9210526315789473\n",
      "--------------------------------------------------\n",
      "Number of selected features: 4\n",
      "Selected features: mean compactness, mean concavity, concavity error, worst concave points\n",
      "SVCSubset accuracy: 0.9298245614035088\n",
      "SVCAll Features Accuracy: 0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "# GaussianNB\n",
    "print(\"-\"*50)\n",
    "task = Task(problem_nb, max_iters=400)\n",
    "\n",
    "algorithm = ArtificialBeeColonyAlgorithm(population_size=80, limit=500)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(feature_names[selected_features].tolist()))\n",
    "features3 = feature_names[selected_features].tolist()\n",
    "\n",
    "model_selected = GaussianNB()\n",
    "model_all = GaussianNB()\n",
    "model_name = \"GaussianNB\"\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print(model_name + 'Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "model_all.fit(X_train, y_train)\n",
    "print(model_name + 'All Features Accuracy:', model_all.score(X_test, y_test))\n",
    "\n",
    "# DecisionTreeClassifier\n",
    "print(\"-\"*50)\n",
    "task = Task(problem_dt, max_iters=400)\n",
    "\n",
    "algorithm = ArtificialBeeColonyAlgorithm(population_size=80, limit=500)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(feature_names[selected_features].tolist()))\n",
    "features3 = feature_names[selected_features].tolist()\n",
    "\n",
    "model_selected = DecisionTreeClassifier()\n",
    "model_all = DecisionTreeClassifier()\n",
    "model_name = \"DecisionTreeClassifier\"\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print(model_name + 'Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "model_all.fit(X_train, y_train)\n",
    "print(model_name + 'All Features Accuracy:', model_all.score(X_test, y_test))\n",
    "\n",
    "# KNeighborsClassifier\n",
    "print(\"-\"*50)\n",
    "task = Task(problem_knn, max_iters=400)\n",
    "\n",
    "algorithm = ArtificialBeeColonyAlgorithm(population_size=80, limit=500)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(feature_names[selected_features].tolist()))\n",
    "features3 = feature_names[selected_features].tolist()\n",
    "\n",
    "model_selected = KNeighborsClassifier()\n",
    "model_all = KNeighborsClassifier()\n",
    "model_name = \"KNeighborsClassifier\"\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print(model_name + 'Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "model_all.fit(X_train, y_train)\n",
    "print(model_name + 'All Features Accuracy:', model_all.score(X_test, y_test))\n",
    "\n",
    "# SVC\n",
    "print(\"-\"*50)\n",
    "task = Task(problem_svc, max_iters=400)\n",
    "\n",
    "algorithm = ArtificialBeeColonyAlgorithm(population_size=80, limit=500)\n",
    "best_features, best_fitness = algorithm.run(task)\n",
    "selected_features = best_features > 0.5\n",
    "print('Number of selected features:', selected_features.sum())\n",
    "print('Selected features:', ', '.join(feature_names[selected_features].tolist()))\n",
    "features3 = feature_names[selected_features].tolist()\n",
    "\n",
    "model_selected = SVC()\n",
    "model_all = SVC()\n",
    "model_name = \"SVC\"\n",
    "\n",
    "model_selected.fit(X_train[:, selected_features], y_train)\n",
    "print(model_name + 'Subset accuracy:', model_selected.score(X_test[:, selected_features], y_test))\n",
    "model_all.fit(X_train, y_train)\n",
    "print(model_name + 'All Features Accuracy:', model_all.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b8e16",
   "metadata": {},
   "source": [
    "# Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82e7ec60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
    "df['target'] = dataset.target\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccf5978c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>worst concave points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.2654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.1860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.2430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.2575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.1625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean compactness  mean concavity  concavity error  worst concave points\n",
       "0           0.27760          0.3001          0.05373                0.2654\n",
       "1           0.07864          0.0869          0.01860                0.1860\n",
       "2           0.15990          0.1974          0.03832                0.2430\n",
       "3           0.28390          0.2414          0.05661                0.2575\n",
       "4           0.13280          0.1980          0.05688                0.1625"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = df.drop(columns=[col for col in df if col not in features3])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54e7498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 4) (569,)\n"
     ]
    }
   ],
   "source": [
    "y =  dataset.target\n",
    "X = df\n",
    "print(X.shape, y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 21)\n",
    "\n",
    "#Feature Scaling, this is important as itt will allow the algorithm to quickly learn a better solution.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc9a66d",
   "metadata": {},
   "source": [
    "## Training a Naive Bayes Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "147a1d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussianNB():\n",
    "    \n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train,y_train)\n",
    "    y_pred_test = gnb.predict(X_test)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    acc = accuracy_score(y_test,y_pred_test)\n",
    "    print(\"Naive Bayes : \", acc)\n",
    "    b_class = [\"benign\", \"malign\"]\n",
    "    print(classification_report(y_test, y_pred_test, target_names=b_class))\n",
    "    \n",
    "    return y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d492b",
   "metadata": {},
   "source": [
    "## Training a Logistic Regression Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "092da955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logisticRegression():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    logreg = LogisticRegression(solver = 'liblinear',multi_class='auto')\n",
    "    logreg.fit(X_train,y_train)\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    acc1 = accuracy_score(y_test,y_pred)\n",
    "    print(\"Logistic Regression: \", acc1)\n",
    "    b_class = [\"benign\", \"malign\"]\n",
    "    print(classification_report(y_test, y_pred, target_names=b_class))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd204e",
   "metadata": {},
   "source": [
    "# Training a Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc3e8c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTrees():\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    dt = DecisionTreeClassifier()\n",
    "    dt.fit(X_train,y_train)\n",
    "    y_pred2 = dt.predict(X_test)\n",
    "    acc2 = accuracy_score(y_test,y_pred2)\n",
    "    print(acc2)\n",
    "    print(\"Decision Trees:\", acc2)\n",
    "    b_class = [\"benign\", \"malign\"]\n",
    "    print(classification_report(y_test, y_pred2, target_names=b_class))\n",
    "\n",
    "    return y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2606e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn():\n",
    "    \n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    clf = KNeighborsClassifier(n_neighbors=8,algorithm='ball_tree')\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred3 = clf.predict(X_test)\n",
    "    acc3 =   accuracy_score(y_test,y_pred3)\n",
    "    print(\"K Nearest Neighbors: \", acc3)\n",
    "    b_class = [\"benign\", \"malign\"]\n",
    "    print(classification_report(y_test, y_pred3, target_names=b_class))  \n",
    "    \n",
    "    return y_pred3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b206ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svc():\n",
    "    \n",
    "    from sklearn.svm import SVC\n",
    "    svc1 = SVC(C=50,kernel='rbf',gamma=1)     \n",
    "    svc1.fit(X_train,y_train)\n",
    "    y_pred4 = svc1.predict(X_test)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    acc4=    accuracy_score(y_test,y_pred4)\n",
    "    print(\"Support Vector Classifier Accuracy\", acc4)\n",
    "    b_class = [\"benign\", \"malign\"]\n",
    "    print(classification_report(y_test, y_pred4, target_names=b_class))  \n",
    "    \n",
    "    return y_pred4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b560aa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes :  0.9122807017543859\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.89      0.85      0.87        39\n",
      "      malign       0.92      0.95      0.93        75\n",
      "\n",
      "    accuracy                           0.91       114\n",
      "   macro avg       0.91      0.90      0.90       114\n",
      "weighted avg       0.91      0.91      0.91       114\n",
      "\n",
      "0.956140350877193\n",
      "Decision Trees: 0.956140350877193\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.95      0.92      0.94        39\n",
      "      malign       0.96      0.97      0.97        75\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.95      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "Logistic Regression:  0.9649122807017544\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.97      0.92      0.95        39\n",
      "      malign       0.96      0.99      0.97        75\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.95      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "Support Vector Classifier Accuracy 0.9473684210526315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.92      0.92      0.92        39\n",
      "      malign       0.96      0.96      0.96        75\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n",
      "K Nearest Neighbors:  0.9473684210526315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.92      0.92      0.92        39\n",
      "      malign       0.96      0.96      0.96        75\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n",
      "\n",
      "Now testing algorithms\n",
      "trained model:  gaussianNB\n",
      "trained model:  decisionTrees\n",
      "trained model:  logisticRegression\n",
      "trained model:  svc\n",
      "trained model:  knn\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = {\n",
    "        \"gaussianNB\": gaussianNB(),\n",
    "        \"decisionTrees\": decisionTrees() ,\n",
    "        \"logisticRegression\": logisticRegression(),\n",
    "        \"svc\": svc(),\n",
    "        \"knn\": knn()\n",
    "    }\n",
    "\n",
    "results = {}\n",
    "print(\"\\nNow testing algorithms\")\n",
    "for algo in models:\n",
    "\n",
    "    print(\"trained model: \", algo )\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# for key, value in models.items():\n",
    "#     results[key] = value\n",
    "#     print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98448bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d4c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b488f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
